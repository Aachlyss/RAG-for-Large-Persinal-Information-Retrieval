{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation using LangChain\n",
    "1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_client = OpenAI()\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Load Document (My mum's book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"data/book_abt_chairs.pdf\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    print(\"The PDF file does not exist!!\")\n",
    "    sys.exit(1)\n",
    "\n",
    "loader = PyPDFLoader(file_path)\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Load embedding model and store PDF as vector in RAM memmory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vector_store = InMemoryVectorStore.from_documents(pages, embedding=embeddings)\n",
    "\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Method for combining the retrieved information so its easier to pass onto the LLM and be more token-efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_doc(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs if doc.page_content])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Expand Query method I added last and adjusted the code accordingly, because the user's question might not match how the information appears in the document and it helps in cases where the user is not 100% sure what they are looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_query(question: str, llm_model) -> list[str]:\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Преформулирай следния въпрос в 5 разнообразни и семантично различни заявки:\\n\\n{question}\"\n",
    "    )\n",
    "    query_chain = RunnablePassthrough() | prompt | llm_model | StrOutputParser()\n",
    "    result = query_chain.invoke(question)\n",
    "    return [q.strip() for q in result.split(\"\\n\") if q.strip()] #Creates a list of 5 queries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Does similarity search for each generated query and removes duplicate texts if there are any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def retrieve_documents(question: str, k: int = 2) -> list[Document]:\n",
    "    expanded_questions = expand_query(question, llm)\n",
    "    all_docs = []\n",
    "    for q in expanded_questions:\n",
    "        docs = vector_store.similarity_search(q, k=k)\n",
    "        all_docs.extend(docs)\n",
    "\n",
    "    seen = set()\n",
    "    unique_docs = []\n",
    "    for doc in all_docs:\n",
    "        if doc.page_content not in seen:    # Remove duplicates\n",
    "            unique_docs.append(doc)\n",
    "            seen.add(doc.page_content)\n",
    "\n",
    "    return unique_docs[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve Info, Format docs into a single string, system message and OpenAI request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Отговори на въпроса САМО на базата на тази информация:\\n\\n{context}\\n\\n\"\n",
    "               \"Ако отговорът не е намерен, кажи 'не мога да отговоря на базата на информацията от документа.'\"),\n",
    "    (\"user\", \"{question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": RunnableLambda(retrieve_documents) | RunnableLambda(format_doc), \"question\": RunnablePassthrough()}    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wrapping this for reusability\n",
    "def answer_question(question: str) -> str:\n",
    "    return rag_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Отговор: Да, в документа се обсъжда темата за ергономия, като се акцентира на добрата ергономия и създаването на среда, която позволява на тялото да се придвижва по различни начини. Специално се споменава концепцията за \"динамично седене\" и как ергономията е важна за проектирането на съвременни столове.\n"
     ]
    }
   ],
   "source": [
    "question = \"Има ли тема за ергономия?\"\n",
    "response = answer_question(question)\n",
    "print(\"Отговор:\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
